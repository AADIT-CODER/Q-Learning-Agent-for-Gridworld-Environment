{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create the gridworld environment (1 = obstacle, 0 = empty space, G = goal)\n",
        "gridworld = np.array([\n",
        "    [0, 0, 0, 1, 0],\n",
        "    [0, 1, 0, 1, 0],\n",
        "    [0, 1, 0, 0, 0],\n",
        "    [0, 0, 0, 1, 0],\n",
        "    [1, 1, 0, 0, 'G']\n",
        "])\n",
        "\n",
        "start_state = (0, 0)\n",
        "goal_state = (4, 4)\n"
      ],
      "metadata": {
        "id": "YK2EqEp6rHRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_space = {\n",
        "    0: 'up',\n",
        "    1: 'down',\n",
        "    2: 'left',\n",
        "    3: 'right'\n",
        "}\n",
        "\n",
        "def is_terminal(state):\n",
        "    return state == goal_state\n",
        "\n",
        "def get_reward(state):\n",
        "    if state == goal_state:\n",
        "        return 100  # Goal\n",
        "    elif gridworld[state] == 1:\n",
        "        return -10  # Obstacle\n",
        "    else:\n",
        "        return -1  # Standard step\n"
      ],
      "metadata": {
        "id": "pmPZOpWNrQth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Exploration rate\n",
        "\n",
        "# Initialize Q-table\n",
        "q_table = np.zeros((gridworld.shape[0], gridworld.shape[1], len(action_space)))\n",
        "\n",
        "# Get next state based on action\n",
        "def next_state(state, action):\n",
        "    x, y = state\n",
        "    if action == 0:  # Up\n",
        "        x = max(x - 1, 0)\n",
        "    elif action == 1:  # Down\n",
        "        x = min(x + 1, gridworld.shape[0] - 1)\n",
        "    elif action == 2:  # Left\n",
        "        y = max(y - 1, 0)\n",
        "    elif action == 3:  # Right\n",
        "        y = min(y + 1, gridworld.shape[1] - 1)\n",
        "\n",
        "    return (x, y)\n",
        "\n",
        "# Choose action using epsilon-greedy policy\n",
        "def choose_action(state):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return random.choice(list(action_space.keys()))  # Explore\n",
        "    else:\n",
        "        return np.argmax(q_table[state])  # Exploit\n",
        "\n",
        "# Q-learning algorithm\n",
        "def q_learning():\n",
        "    for episode in range(1000):  # Train for 1000 episodes\n",
        "        state = start_state\n",
        "\n",
        "        while not is_terminal(state):\n",
        "            action = choose_action(state)\n",
        "            next_s = next_state(state, action)\n",
        "            reward = get_reward(next_s)\n",
        "\n",
        "            # Q-value update\n",
        "            q_table[state][action] = q_table[state][action] + alpha * (\n",
        "                reward + gamma * np.max(q_table[next_s]) - q_table[state][action]\n",
        "            )\n",
        "\n",
        "            # Update state\n",
        "            state = next_s\n",
        "\n",
        "# Train the agent\n",
        "q_learning()\n"
      ],
      "metadata": {
        "id": "RktPd9TarVnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_agent():\n",
        "    state = start_state\n",
        "    steps = 0\n",
        "    path = [state]\n",
        "\n",
        "    while not is_terminal(state):\n",
        "        action = np.argmax(q_table[state])\n",
        "        state = next_state(state, action)\n",
        "        path.append(state)\n",
        "        steps += 1\n",
        "\n",
        "    print(f\"Agent reached the goal in {steps} steps!\")\n",
        "    print(\"Path taken:\", path)\n",
        "\n",
        "# Test the agent\n",
        "test_agent()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtkqpgLirYJH",
        "outputId": "14d4d4eb-6d87-4d69-9d40-a229f058cb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent reached the goal in 8 steps!\n",
            "Path taken: [(0, 0), (0, 1), (0, 2), (1, 2), (2, 2), (3, 2), (3, 3), (3, 4), (4, 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " #Limitations and Possible Extensions\n",
        "\n",
        "ðŸ˜­Limitations:\n",
        "\n",
        "1.Q-learning requires significant time for large environments.\n",
        "2.Does not work well with continuous state/action spaces.\n",
        "3.Exploration-exploitation trade-off can be tricky.\n",
        "\n",
        "ðŸ˜ƒPossible Extensions:\n",
        "\n",
        "1.Implement Deep Q-Networks (DQN) to handle continuous spaces.\n",
        "2.Use dynamic obstacles or changing environments to make the problem more complex."
      ],
      "metadata": {
        "id": "zf0vfJN5reQ7"
      }
    }
  ]
}